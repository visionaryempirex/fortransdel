def get_ai_response2(prompt):
    # Your existing implementation
    client = Groq(
        api_key=os.getenv("GROQ_API_KEY"),
    )

    llm = client.chat.completions.create(
        model="llama3-8b-8192",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2
    )

    response = llm.choices[0].message.content
    return response
